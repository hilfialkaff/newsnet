\section{Evaluation}
\label{sec:eval}

In this section, we begin with describing the datasets used in our experiments
before evaluating the performance of \h on those datasets.

\subsection{Datasets}

We test \h on two different datasets.

% The DBLP dataset is a network of Authors, Papers, Citations, Venues, and
% Terms. It is a heterogeneous network, however it does not contain any
% hierarchical entities. The total size of the data is 1.1GB, the relationship
% between entities were as follow: Author$\leftrightarrow$Paper,
% Paper$\leftrightarrow$Term, and Paper$\leftrightarrow$Venue.

The DBLP Hierarchical Dataset is similar to the previous one. Except, that now
the relations are as follows: Paper$\leftrightarrow$Area,
Author$\leftrightarrow$Area, Conf$\leftrightarrow$Area,
Term$\leftrightarrow$Area.  Furthermore, Area and Conf entities form a
hierarchy.  This dataset is much more smaller than the previous one, only
7.9MB. There are 70536 vertices in this dataset with each node having
an average degree of 9.0 with a standard deviation of 74.8. The average
path length is 4.0 with a standard deviation of 1.10.

Finally, the last dataset is from the New York Times. It represents a
comprehensive dataset that is both heteregenous and also hierarchical. The
network is 650MB, with entities: Article, Location, Organization, Person,
Topic. Furthermore, each one of these entites except Article has its own
hierarchy. There are 181874 vertices in this dataset with each node having an
average degree of 10.0 and a standard deviation of 73.7. The average path length
is 4 with a standard deviation of 0.97.

\subsection {Experiments}

There are three main metrics that we would like to evaluate the performance of
our \h framework on: the runtimes similarity search query, drill-down and
roll-up OLAP queries. Since the underlying similarity search algorithm used is
the same as PathSim, the accuracy of \h is also the same and thus, we do not
display the comparison.

\begin{table}
    \centering
    \begin{tabular}{| l | l | l |}
        \hline
        Graph & Average (s) & Stddev \\ \hline
        DBLP Full & 5.26 & 0.042 \\ \hline
        NYT Full & 5.12 & 0.157 \\ \hline
        DBLP DB & 1.41 & 0.03 \\ \hline
        NYT Jordan & 1.70 & 0.04 \\
        \hline
    \end{tabular}

    \caption{Average time taken and standard deviation for similarity search on random pairs of entities in the DBLP and NYTimes datasets.}
    \label{tab:similarity-result}
\end{table}

For the similarity search, we measure the times it takes to
execute similarity search on a pair of nodes with meta-path length of 4. The
evaluation was repeated 5 times. Table~\ref{tab:similarity-result} shows
the time taken on 4 graphs. The first two are
the full DBLP dataset and the full NYTimes dataset while
the last two are a subgraph of the DBLP dataset whose
vertices' ''area`` dimension is database and a subgraph of the NYT
datasets whose vertices' ''country`` dimension is Jordan.
We can see that although, we increase the graph size considerably with
NYT Full our runtimes are about the same. We can also see that
the standard deviation for similarity search is low, hence our results
should be stable.

\begin{table}
    \centering
    \begin{tabular}{| l | l | l |}
        \hline
        Graph & Average (s) & Stddev \\ \hline
        DBLP drill down & 0.055 & 0.024 \\ \hline
        DBLP roll up & 0.0058 & 0.00093 \\ \hline
        NYT drill down & 0.52 & 0.06 \\ \hline
        NYT roll up & 19.22 & 0.91 \\
        \hline
    \end{tabular}

    \caption{Average time taken and standard deviation to perform drill-down
    and roll-up on the DBLP and NYT datasets.}
    \label{tab:similarity-time}
\end{table}

For the OLAP operations, we measure the time it takes to
perform drill-down and roll-up under specific categories.
For the DBLP data the drilled-down subgraph is four times smaller
than the original graph. On the other hand, there was no subgraph in NYT 
that would allow us to get a network of size four times smaller. So we resorted to 
a network that was only 10 \% smaller than the original one.
 Table~\ref{tab:similarity-result} shows
the time taken for OLAP operations on both the DBLP and NYTimes dataset.
We can see that the drill-down operations are really performant, however roll-up can
take a long amount of time when we are dealing with a very large network.
