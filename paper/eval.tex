\section{Evaluation}
\label{sec:eval}

In this section, we begin with describing the datasets used in our experiments
before evaluating the performance of \h on those datasets.

\subsection{Datasets}

We test \h on two different datasets.

% The DBLP dataset is a network of Authors, Papers, Citations, Venues, and
% Terms. It is a heterogeneous network, however it does not contain any
% hierarchical entities. The total size of the data is 1.1GB, the relationship
% between entities were as follow: Author$\leftrightarrow$Paper,
% Paper$\leftrightarrow$Term, and Paper$\leftrightarrow$Venue.

The DBLP Hierarchical Dataset is similar to the previous one. Except, that now
the relations are as follows: Paper$\leftrightarrow$Area,
Author$\leftrightarrow$Area, Conf$\leftrightarrow$Area,
Term$\leftrightarrow$Area.  Furthermore, Area and Conf entities form a
hierarchy.  This dataset is much more smaller than the previous one, only
7.9MB. There are 70536 vertices in this dataset with each node having
an average degree of 9.0 with a standard deviation of 74.8. The average
path length is 4.0 with a standard deviation of 1.10.

Finally, the last dataset is from the New York Times. It represents a
comprehensive dataset that is both heteregenous and also hierarchical. The
network is 650MB, with entities: Article, Location, Organization, Person,
Topic. Furthermore, each one of these entites except Article has its own
hierarchy. There are 181874 vertices in this dataset with each node having an
average degree of 10.0 and a standard deviation of 73.7. The average path length
is 4 with a standard deviation of 0.97.

\subsection {Experiments}

There are three main metrics that we would like to evaluate the performance of
our \h framework on: the runtimes similarity search query, drill-down and
roll-up OLAP queries. Since the underlying similarity search algorithm used is
the same as PathSim, the accuracy of \h is also the same and thus, we do not
display the comparison.

\begin{table}
    \centering
    \begin{tabular}{| l | l | l |}
        \hline
        Graph & Average (s) & Stddev \\ \hline
        DBLP Full & 1.02 & 2.70 \\ \hline
        NYT Full & 12.25 & 10.31 \\ \hline
        DBLP DB & 0.21 & 0.27 \\ \hline
        NYT Jordan & 7.34 & 7.35 \\
        \hline
    \end{tabular}

    \caption{Average time taken and standard deviation for similarity search on random pairs of entities in the DBLP and NYTimes datasets.}
    \label{tab:similarity-result}
\end{table}

For the similarity search, we measure the times it takes to
execute similarity search on 100 pairs of nodes chosen
randomly from the graph. Table~\ref{tab:similarity-result} shows
the time taken on 4 graphs. The first two are
the full DBLP dataset and the full NYTimes dataset while
the last two are a subgraph of the DBLP dataset whose
vertices' ''area`` dimension is database and a subgraph of the NYT
datasets whose vertices' ''country`` dimension is Jordan.

\begin{table}
    \centering
    \begin{tabular}{| l | l | l |}
        \hline
        Graph & Average (s) & Stddev \\ \hline
        DBLP drill down & 0.055 & 0.024 \\ \hline
        DBLP roll up & 0.0058 & 0.00093 \\ \hline
        NYT drill down & 5.70 & 0.44 \\ \hline
        NYT roll up & 19.22 & 0.91 \\
        \hline
    \end{tabular}

    \caption{Average time taken and standard deviation to perform drill-down
    and roll-up on the DBLP and NYT datasets.}
    \label{tab:similarity-time}
\end{table}

For the OLAP operations, we measure the time it takes to
perform drill-down and roll-up under a random category
that is present in the dataset. Table~\ref{tab:similarity-result} shows
the time taken for OLAP operations on both the DBLP and NYTimes dataset.
